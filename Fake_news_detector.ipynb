{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news = pd.read_csv('D:/kaggle_fake_news/news/True.csv')\n",
    "fake_news = pd.read_csv('D:/kaggle_fake_news/news/Fake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news['Fake'] = 0\n",
    "fake_news['Fake']  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([real_news,fake_news])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  Fake  \n",
       "0  December 31, 2017      0  \n",
       "1  December 29, 2017      0  \n",
       "2  December 31, 2017      0  \n",
       "3  December 30, 2017      0  \n",
       "4  December 29, 2017      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Fake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23476</th>\n",
       "      <td>McPain: John McCain Furious That Iran Treated ...</td>\n",
       "      <td>21st Century Wire says As 21WIRE reported earl...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 16, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23477</th>\n",
       "      <td>JUSTICE? Yahoo Settles E-mail Privacy Class-ac...</td>\n",
       "      <td>21st Century Wire says It s a familiar theme. ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 16, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23478</th>\n",
       "      <td>Sunnistan: US and Allied ‘Safe Zone’ Plan to T...</td>\n",
       "      <td>Patrick Henningsen  21st Century WireRemember ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 15, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23479</th>\n",
       "      <td>How to Blow $700 Million: Al Jazeera America F...</td>\n",
       "      <td>21st Century Wire says Al Jazeera America will...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 14, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23480</th>\n",
       "      <td>10 U.S. Navy Sailors Held by Iranian Military ...</td>\n",
       "      <td>21st Century Wire says As 21WIRE predicted in ...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>January 12, 2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "23476  McPain: John McCain Furious That Iran Treated ...   \n",
       "23477  JUSTICE? Yahoo Settles E-mail Privacy Class-ac...   \n",
       "23478  Sunnistan: US and Allied ‘Safe Zone’ Plan to T...   \n",
       "23479  How to Blow $700 Million: Al Jazeera America F...   \n",
       "23480  10 U.S. Navy Sailors Held by Iranian Military ...   \n",
       "\n",
       "                                                    text      subject  \\\n",
       "23476  21st Century Wire says As 21WIRE reported earl...  Middle-east   \n",
       "23477  21st Century Wire says It s a familiar theme. ...  Middle-east   \n",
       "23478  Patrick Henningsen  21st Century WireRemember ...  Middle-east   \n",
       "23479  21st Century Wire says Al Jazeera America will...  Middle-east   \n",
       "23480  21st Century Wire says As 21WIRE predicted in ...  Middle-east   \n",
       "\n",
       "                   date  Fake  \n",
       "23476  January 16, 2016     1  \n",
       "23477  January 16, 2016     1  \n",
       "23478  January 15, 2016     1  \n",
       "23479  January 14, 2016     1  \n",
       "23480  January 12, 2016     1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating folds\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "df = pd.read_csv('D:/kaggle_fake_news/news/Dataset.csv')\n",
    "#creating new column called kfold\n",
    "df ['kfold'] = -1\n",
    "\n",
    "#next step is to randomize rows of the data\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop = True)\n",
    "\n",
    "#fetch labels or targetvalues\n",
    "\n",
    "y = df.Fake.values\n",
    "\n",
    "#intiating the Kfold class from model_selection\n",
    "\n",
    "kf = model_selection.StratifiedKFold(n_splits = 5)\n",
    "\n",
    "#fill the new kfold column\n",
    "\n",
    "for f, (t_,v_) in enumerate(kf.split(X=df,y=y)):\n",
    "    df.loc[v_,'kfold'] = f\n",
    "\n",
    "#saving new csv with kfold column\n",
    "\n",
    "df.to_csv('dataset_folds.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/kaggle_fake_news/news/dataset_folds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>Fake</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13869</td>\n",
       "      <td>WEDDING CRASHERS: Hillary Tries To Explain Why...</td>\n",
       "      <td>Who goes to a wedding and doesn t bring a gift...</td>\n",
       "      <td>politics</td>\n",
       "      <td>May 20, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11945</td>\n",
       "      <td>At least 65 media workers killed doing their j...</td>\n",
       "      <td>BERLIN (Reuters) - At least 65 media workers a...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>December 19, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12832</td>\n",
       "      <td>Erdogan, Putin say U.S. decision on Jerusalem ...</td>\n",
       "      <td>ANKARA (Reuters) - Turkish President Tayyip Er...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>December 7, 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13429</td>\n",
       "      <td>HERE’S HOW HILLARY’S VP PICK Has Just Proven H...</td>\n",
       "      <td>Hillary s VP pick is proving himself to be a a...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Jul 26, 2016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6749</td>\n",
       "      <td>Fed turns to Trump agenda with rate hike nearl...</td>\n",
       "      <td>WASHINGTON (Reuters) - The Federal Reserve ina...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 12, 2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0       13869  WEDDING CRASHERS: Hillary Tries To Explain Why...   \n",
       "1       11945  At least 65 media workers killed doing their j...   \n",
       "2       12832  Erdogan, Putin say U.S. decision on Jerusalem ...   \n",
       "3       13429  HERE’S HOW HILLARY’S VP PICK Has Just Proven H...   \n",
       "4        6749  Fed turns to Trump agenda with rate hike nearl...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  Who goes to a wedding and doesn t bring a gift...      politics   \n",
       "1  BERLIN (Reuters) - At least 65 media workers a...     worldnews   \n",
       "2  ANKARA (Reuters) - Turkish President Tayyip Er...     worldnews   \n",
       "3  Hillary s VP pick is proving himself to be a a...      politics   \n",
       "4  WASHINGTON (Reuters) - The Federal Reserve ina...  politicsNews   \n",
       "\n",
       "                 date  Fake  kfold  \n",
       "0        May 20, 2016     1      0  \n",
       "1  December 19, 2017      0      0  \n",
       "2   December 7, 2017      0      0  \n",
       "3        Jul 26, 2016     1      0  \n",
       "4  December 12, 2016      0      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['title', 'subject','date'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create a simple dataset class Dataset class returns one sample of the training or validation data. \n",
    "\n",
    "import torch\n",
    "\n",
    "class NEWSData:\n",
    "    def __init__(self,texts,targets):\n",
    "        self.texts = texts\n",
    "        self.target = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        #for any given item, which is an int,  \n",
    "        #return review and targets as torch tensor  \n",
    "        #item is the index of the item in concern\n",
    "        text = self.texts[item,:]\n",
    "        target = self.target[item]\n",
    "        \n",
    "        return{\n",
    "            'text': torch.tensor(text,dtype=torch.long),\n",
    "            'target':torch.tensor(target,dtype = torch.float)            \n",
    "        }\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can create lstm class which consists of our LSTM  model. \n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(LSTM,self).__init__()\n",
    "        #number of words = number of rows in embedding matrix\n",
    "        num_words = embedding_matrix.shape[0]\n",
    "        \n",
    "        #dimension of embedding matrix is num of columns in matrix\n",
    "        \n",
    "        embed_dim = embedding_matrix.shape[1]\n",
    "        \n",
    "        #we define an input embedding layer\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=num_words,\n",
    "            embedding_dim= embed_dim\n",
    "        )\n",
    "        \n",
    "        #embedding matrix is used as weights of the embedding layer\n",
    "        \n",
    "        self.embedding.weight = nn.Parameter(\n",
    "            torch.tensor(\n",
    "            embedding_matrix, dtype = torch.float32)\n",
    "        )\n",
    "        \n",
    "        #we dont want to train the pretrained embedding\n",
    "        \n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        #a simple bidirectional LSTM with hidden size of 128\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,128, bidirectional=True, batch_first=True,\n",
    "        )\n",
    "        \n",
    "        #output layer which is a linear layer we have only one output\n",
    "        \n",
    "        self.out = nn.Linear(512,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #passing data through embedding layer\n",
    "        #input is tokens\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        #moving embedding output to lstm\n",
    "        \n",
    "        x,_ = self.lstm(x)\n",
    "        \n",
    "        #apply mean and max pooling on lstm output\n",
    "        \n",
    "        avg_pool = torch.mean(x,1)\n",
    "        max_pool, _  = torch.max(x,1)\n",
    "        \n",
    "        #concat mean and max pool \n",
    "        #this is why the size is 512\n",
    "        #128 for each direction\n",
    "        #avg_pool = 256 and max_pool = 256\n",
    "        \n",
    "        out = torch.cat((avg_pool,max_pool),1)\n",
    "        \n",
    "        #pass through the output layer and return the output\n",
    "        \n",
    "        out = self.out(out)\n",
    "        \n",
    "        #return linear output\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we create  our training and evaluation functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader,model,optimizer,device):\n",
    "    #set model to training mode\n",
    "    print('setting model to training mode')\n",
    "    model.train()\n",
    "    for data in data_loader:\n",
    "        texts = data['text']\n",
    "        targets = data['target']\n",
    "        \n",
    "        #move the data to device\n",
    "        \n",
    "        texts = texts.to(device, dtype = torch.long)\n",
    "        targets  = targets.to(device, dtype = torch.float32)\n",
    "        \n",
    "        #clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #make predictions from model\n",
    "        \n",
    "        predictions = model(texts)\n",
    "        \n",
    "        #calc loss\n",
    "        \n",
    "        loss = nn.BCEWithLogitsLoss()(\n",
    "        predictions,targets.view(-1,1))\n",
    "        \n",
    "        #compute grad wrt all parameters of model that are trainable\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #single optimization step\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "def evaluate(data_loader,model,device):\n",
    "    #initialize list to store prediction and targets\n",
    "    \n",
    "    final_predictions = []\n",
    "    final_targets = []\n",
    "    \n",
    "    #put model in eval mode\n",
    "    \n",
    "    model.eval()\n",
    "    #disable gradient cal\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            texts = data['text']\n",
    "            targets = data['target']\n",
    "            \n",
    "            \n",
    "            texts = texts.to(device, dtype = torch.long)\n",
    "            \n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            #make predictions\n",
    "            \n",
    "            predictions = model(texts)\n",
    "            \n",
    "            #move prediction and target to list\n",
    "            \n",
    "            predictions = predictions.cpu().numpy().tolist()\n",
    "            targets = data['target'].cpu().numpy().tolist()\n",
    "            final_predictions.extend(predictions)\n",
    "            final_targets.extend(targets)\n",
    "    \n",
    "    return final_predictions,final_targets\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions will help us in training multiple folds. \n",
    "import io\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname,'r',errors = 'ignore')\n",
    "    \n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        values = line.split(' ')\n",
    "        word = values[0] ## The first entry is the word\n",
    "        coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "        data[word] = coefs\n",
    "    return data\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index, embedding_dict):\n",
    "    #intialize matrix with zeros\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_index)+1,300))\n",
    "    \n",
    "    #loop over all the words\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        #if word is found in pretrained embedding update the matrix\n",
    "        #if not found the vector is zero\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[i] = embedding_dict[word]\n",
    "    #return embedding matrix\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "\n",
    "def run(df,fold):\n",
    "    train_df = df[df.kfold != fold].reset_index(drop = True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop = True)\n",
    "    \n",
    "    print('Fitting tokenizer')\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(df.text.values.tolist())\n",
    "    \n",
    "    xtrain = tokenizer.texts_to_sequences(train_df.text.values)\n",
    "    xtest = tokenizer.texts_to_sequences(valid_df.text.values)\n",
    "    \n",
    "    \n",
    "    #zero padding the training seq given the max length this padding is done on left side\n",
    "    xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain, maxlen=MAX_LEN)\n",
    "    \n",
    "    xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest, maxlen=MAX_LEN)\n",
    "    \n",
    "    #initialize dataset class for training\n",
    "    \n",
    "    train_dataset = NEWSData(\n",
    "        texts = xtrain,\n",
    "        targets = train_df.Fake.values\n",
    "    )\n",
    "    \n",
    "    #create torh data loader for training \n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=TRAIN_BATCH_SIZE,\n",
    "                    num_workers = 0\n",
    "                    )\n",
    "    \n",
    "    #initialize dataset class for validation\n",
    "    \n",
    "    valid_dataset = NEWSData(\n",
    "        texts = xtest,\n",
    "        targets = valid_df.Fake.values\n",
    "    )\n",
    "    \n",
    "    #create torh data loader for validation \n",
    "    \n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "                    valid_dataset,\n",
    "                    batch_size=VALID_BATCH_SIZE,\n",
    "                    num_workers = 0\n",
    "                    )\n",
    "    \n",
    "    print('Loading Embedding')\n",
    "    \n",
    "    embedding_dict = load_vectors('D:/kaggle_fake_news/glove.6B/glove.6B.300d.txt')\n",
    "    embedding_matrix = create_embedding_matrix(\n",
    "            \n",
    "        tokenizer.word_index,embedding_dict)\n",
    "    \n",
    "    #create torch device\n",
    "    \n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    #fetching our lstm model\n",
    "    \n",
    "    model = LSTM(embedding_matrix)\n",
    "    \n",
    "    #send model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    #initialize adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    \n",
    "    print('training model')\n",
    "    \n",
    "    \n",
    "    #set best accuracy to zero\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    \n",
    "    #set early stopping counter to zero\n",
    "    \n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train(train_data_loader,model,optimizer,device)\n",
    "        \n",
    "        outputs, targets = evaluate(valid_data_loader,model,device)\n",
    "        \n",
    "        outputs = np.array(outputs) >= 0.5\n",
    "        #calc accuracy\n",
    "        \n",
    "        accuracy = metrics.accuracy_score(targets,outputs)\n",
    "        \n",
    "        print(f'FOLD:{fold},EPOCH:{epoch},Accuracy Score = {accuracy}')\n",
    "        \n",
    "        #simple early stopping\n",
    "        \n",
    "        if accuracy>best_accuracy:\n",
    "            torch.save(model.state_dict(),\"model.bin\")\n",
    "            best_accuracy = accuracy\n",
    "        else:\n",
    "            early_stopping_counter +=1\n",
    "        if early_stopping_counter >2:\n",
    "            break\n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer\n",
      "Loading Embedding\n",
      "training model\n",
      "setting model to training mode\n",
      "FOLD:0,EPOCH:0,Accuracy Score = 0.983630289532294\n",
      "setting model to training mode\n",
      "FOLD:0,EPOCH:1,Accuracy Score = 0.9902004454342984\n",
      "setting model to training mode\n",
      "FOLD:0,EPOCH:2,Accuracy Score = 0.9906458797327394\n",
      "setting model to training mode\n",
      "FOLD:0,EPOCH:3,Accuracy Score = 0.9925389755011136\n",
      "setting model to training mode\n",
      "FOLD:0,EPOCH:4,Accuracy Score = 0.9927616926503341\n",
      "setting model to training mode\n",
      "FOLD:0,EPOCH:5,Accuracy Score = 0.9933184855233853\n",
      "setting model to training mode\n",
      "FOLD:0,EPOCH:6,Accuracy Score = 0.9934298440979955\n",
      "setting model to training mode\n",
      "FOLD:0,EPOCH:7,Accuracy Score = 0.9920935412026726\n",
      "setting model to training mode\n",
      "FOLD:0,EPOCH:8,Accuracy Score = 0.9898663697104677\n",
      "setting model to training mode\n",
      "FOLD:0,EPOCH:9,Accuracy Score = 0.994097995545657\n"
     ]
    }
   ],
   "source": [
    "run(df,fold = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer\n",
      "Loading Embedding\n",
      "training model\n",
      "setting model to training mode\n",
      "FOLD:1,EPOCH:0,Accuracy Score = 0.9822939866369711\n",
      "setting model to training mode\n",
      "FOLD:1,EPOCH:1,Accuracy Score = 0.9893095768374165\n",
      "setting model to training mode\n",
      "FOLD:1,EPOCH:2,Accuracy Score = 0.9908685968819599\n",
      "setting model to training mode\n",
      "FOLD:1,EPOCH:3,Accuracy Score = 0.9926503340757238\n",
      "setting model to training mode\n",
      "FOLD:1,EPOCH:4,Accuracy Score = 0.9919821826280624\n",
      "setting model to training mode\n",
      "FOLD:1,EPOCH:5,Accuracy Score = 0.9929844097995546\n",
      "setting model to training mode\n",
      "FOLD:1,EPOCH:6,Accuracy Score = 0.9934298440979955\n",
      "setting model to training mode\n",
      "FOLD:1,EPOCH:7,Accuracy Score = 0.9934298440979955\n",
      "setting model to training mode\n",
      "FOLD:1,EPOCH:8,Accuracy Score = 0.9914253897550112\n"
     ]
    }
   ],
   "source": [
    "run(df,fold = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer\n",
      "Loading Embedding\n",
      "training model\n",
      "setting model to training mode\n",
      "FOLD:2,EPOCH:0,Accuracy Score = 0.9850779510022272\n",
      "setting model to training mode\n",
      "FOLD:2,EPOCH:1,Accuracy Score = 0.9907572383073496\n",
      "setting model to training mode\n",
      "FOLD:2,EPOCH:2,Accuracy Score = 0.9927616926503341\n",
      "setting model to training mode\n",
      "FOLD:2,EPOCH:3,Accuracy Score = 0.9910913140311804\n",
      "setting model to training mode\n",
      "FOLD:2,EPOCH:4,Accuracy Score = 0.9929844097995546\n",
      "setting model to training mode\n",
      "FOLD:2,EPOCH:5,Accuracy Score = 0.9927616926503341\n",
      "setting model to training mode\n",
      "FOLD:2,EPOCH:6,Accuracy Score = 0.9929844097995546\n"
     ]
    }
   ],
   "source": [
    "run(df,fold = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer\n",
      "Loading Embedding\n",
      "training model\n",
      "setting model to training mode\n",
      "FOLD:3,EPOCH:0,Accuracy Score = 0.9842966922819913\n",
      "setting model to training mode\n",
      "FOLD:3,EPOCH:1,Accuracy Score = 0.9913130638155697\n",
      "setting model to training mode\n",
      "FOLD:3,EPOCH:2,Accuracy Score = 0.9915358057690166\n",
      "setting model to training mode\n",
      "FOLD:3,EPOCH:3,Accuracy Score = 0.9919812896759105\n",
      "setting model to training mode\n",
      "FOLD:3,EPOCH:4,Accuracy Score = 0.9930949994431452\n",
      "setting model to training mode\n",
      "FOLD:3,EPOCH:5,Accuracy Score = 0.9914244347922931\n",
      "setting model to training mode\n",
      "FOLD:3,EPOCH:6,Accuracy Score = 0.9929836284664216\n",
      "setting model to training mode\n",
      "FOLD:3,EPOCH:7,Accuracy Score = 0.9925381445595278\n"
     ]
    }
   ],
   "source": [
    "run(df,fold = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tokenizer\n",
      "Loading Embedding\n",
      "training model\n",
      "setting model to training mode\n",
      "FOLD:4,EPOCH:0,Accuracy Score = 0.9824033856776924\n",
      "setting model to training mode\n",
      "FOLD:4,EPOCH:1,Accuracy Score = 0.9879719345138657\n",
      "setting model to training mode\n",
      "FOLD:4,EPOCH:2,Accuracy Score = 0.991869918699187\n",
      "setting model to training mode\n",
      "FOLD:4,EPOCH:3,Accuracy Score = 0.9922040316293574\n",
      "setting model to training mode\n",
      "FOLD:4,EPOCH:4,Accuracy Score = 0.9912016928388462\n",
      "setting model to training mode\n",
      "FOLD:4,EPOCH:5,Accuracy Score = 0.9904220960017819\n",
      "setting model to training mode\n",
      "FOLD:4,EPOCH:6,Accuracy Score = 0.993540483350039\n",
      "setting model to training mode\n",
      "FOLD:4,EPOCH:7,Accuracy Score = 0.9928722574896982\n"
     ]
    }
   ],
   "source": [
    "run(df,fold = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
